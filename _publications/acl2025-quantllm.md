---
title: "One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments"
collection: publications
permalink: /publication/acl2025-quantllm
excerpt: 'A unified approach for fine-tuning quantized LLMs that enables efficient deployment across different scenarios.'
date: 2025-02-02
venue: 'ACL'
citation: '<b>Ke Yi</b>, Yuhui Xu, Heng Chang, Chen Tang, Yuan Meng, Tong Zhang, Jia Li. <b>ACL 2025 Oral</b>'
---

## Abstract

We propose a unified fine-tuning approach for quantized large language models that enables efficient deployment across various scenarios. Our method achieves state-of-the-art performance while maintaining deployment efficiency.

## Key Contributions

- Unified quantization-aware fine-tuning framework
- Efficient deployment across multiple scenarios
- Selected for **Oral presentation** at ACL 2025

