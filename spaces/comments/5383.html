<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>变分自编码器（三）：这样做为什么能成？ - 评论归档</title>
    
    <!-- MathJax 配置 -->
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
    
    <style>
        :root {
            --bg-primary: #0f0f0f;
            --bg-secondary: #1a1a1a;
            --bg-tertiary: #252525;
            --bg-hover: #2a2a2a;
            --text-primary: #e8e8e8;
            --text-secondary: #a0a0a0;
            --text-muted: #666;
            --accent: #4a9eff;
            --accent-dim: #3a7acc;
            --border: #333;
            --reply-border: #444;
            --author-color: #6cb6ff;
            --author-admin: #f0a040;
            --date-color: #7a7a7a;
            --layer-bg: #2d4a5e;
            --layer-reply-bg: #3d5a3e;
        }
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Noto Sans SC", sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.7;
            min-height: 100vh;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
        }
        
        header {
            text-align: center;
            padding: 2rem 0 3rem;
            border-bottom: 1px solid var(--border);
            margin-bottom: 2rem;
        }
        
        h1 {
            font-size: 1.8rem;
            font-weight: 600;
            margin-bottom: 1rem;
            color: var(--text-primary);
            line-height: 1.4;
        }
        
        h1 a {
            color: var(--accent);
            text-decoration: none;
        }
        
        h1 a:hover {
            text-decoration: underline;
        }
        
        .meta {
            color: var(--text-secondary);
            font-size: 0.95rem;
        }
        
        .meta span {
            margin: 0 0.5rem;
        }
        
        .stats {
            display: inline-flex;
            gap: 1.5rem;
            margin-top: 1rem;
            padding: 0.75rem 1.5rem;
            background: var(--bg-secondary);
            border-radius: 8px;
        }
        
        .stat {
            text-align: center;
        }
        
        .stat-value {
            font-size: 1.5rem;
            font-weight: 700;
            color: var(--accent);
        }
        
        .stat-label {
            font-size: 0.8rem;
            color: var(--text-muted);
        }
        
        .comment-thread {
            background: var(--bg-secondary);
            border-radius: 12px;
            margin-bottom: 1.5rem;
            overflow: hidden;
            border: 1px solid var(--border);
        }
        
        .comment {
            padding: 1.25rem 1.5rem;
            border-bottom: 1px solid var(--border);
        }
        
        .comment:last-child {
            border-bottom: none;
        }
        
        .comment.reply {
            background: var(--bg-tertiary);
            margin-left: 0;
            padding-left: 2rem;
            border-left: 3px solid var(--reply-border);
        }
        
        .comment.reply-deep {
            background: var(--bg-hover);
            padding-left: 2.5rem;
        }
        
        .comment-header {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            margin-bottom: 0.75rem;
            flex-wrap: wrap;
        }
        
        .author {
            font-weight: 600;
            color: var(--author-color);
            text-decoration: none;
        }
        
        .author.admin {
            color: var(--author-admin);
        }
        
        .author:hover {
            text-decoration: underline;
        }
        
        .layer {
            font-size: 0.75rem;
            padding: 0.2rem 0.5rem;
            background: var(--layer-bg);
            color: #fff;
            border-radius: 4px;
            font-weight: 500;
        }
        
        .layer.reply {
            background: var(--layer-reply-bg);
        }
        
        .date {
            font-size: 0.85rem;
            color: var(--date-color);
            margin-left: auto;
        }
        
        .comment-content {
            color: var(--text-primary);
            font-size: 0.95rem;
        }
        
        .comment-content p {
            margin-bottom: 0.75rem;
        }
        
        .comment-content p:last-child {
            margin-bottom: 0;
        }
        
        .comment-content blockquote {
            border-left: 3px solid var(--accent-dim);
            padding-left: 1rem;
            margin: 0.75rem 0;
            color: var(--text-secondary);
            background: var(--bg-primary);
            padding: 0.75rem 1rem;
            border-radius: 0 8px 8px 0;
        }
        
        .comment-content a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .comment-content a:hover {
            text-decoration: underline;
        }
        
        .comment-content code {
            background: var(--bg-tertiary);
            padding: 0.15rem 0.4rem;
            border-radius: 4px;
            font-size: 0.9em;
        }
        
        .comment-content br {
            display: block;
            margin-bottom: 0.5rem;
            content: "";
        }
        
        /* MathJax 样式优化 */
        .MathJax {
            font-size: 1.1em !important;
        }
        
        mjx-container {
            overflow-x: auto;
            overflow-y: hidden;
            padding: 0.25rem 0;
        }
        
        footer {
            text-align: center;
            padding: 2rem;
            color: var(--text-muted);
            font-size: 0.85rem;
            border-top: 1px solid var(--border);
            margin-top: 2rem;
        }
        
        footer a {
            color: var(--accent);
            text-decoration: none;
        }
        
        .no-comments {
            text-align: center;
            padding: 3rem;
            color: var(--text-muted);
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 1rem;
            }
            
            h1 {
                font-size: 1.4rem;
            }
            
            .comment {
                padding: 1rem;
            }
            
            .comment.reply {
                padding-left: 1.25rem;
            }
            
            .date {
                width: 100%;
                margin-left: 0;
                margin-top: 0.5rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1><a href="https://spaces.ac.cn/archives/5383" target="_blank">变分自编码器（三）：这样做为什么能成？</a></h1>
            <p class="meta">
                <span>评论归档</span>
                <span>•</span>
                <span>生成于 2025-12-29 20:32</span>
            </p>
            <div class="stats">
                <div class="stat">
                    <div class="stat-value">77</div>
                    <div class="stat-label">条评论</div>
                </div>
            </div>
        </header>
        
        <main>
<div class="comment-thread">
    <div class="comment" id="comment-8973">
        <div class="comment-header">
            <span class="author">cr7</span>
            <span class="layer">楼主</span>
            <span class="date">April 4th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-8973">
<p>大神好，有个图好像显示不出来：标准正态分布（蓝）和小方差正态分布（橙）</p> 
</div>
        </div>
    </div>
    <div class="comment reply" id="comment-8974">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第2层</span>
            <span class="date">April 4th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-8974">
<p>刷新或者右击在新窗口打开？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-8975">
        <div class="comment-header">
            <span class="author">cr7</span>
            <span class="layer reply">第3层</span>
            <span class="date">April 4th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-8975">
<p>谢谢啦</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-9006">
        <div class="comment-header">
            <span class="author">Mobil</span>
            <span class="layer">第2层</span>
            <span class="date">April 13th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-9006">
<p>苏老师，最近看到文章用几何（微分几何）的观点来阐述和改进深度学习模型，可否请您看看点评下：<br/>
http://mp.weixin.qq.com/s/hWGxXALRBqdpUES7RMwAtA<br/>
Na Lei,Kehua Su,Li Cui,Shing-Tung Yau,David Xianfeng Gu, A Geometric View of Optimal Transportation and Generative Model, arXiv:1710.05488.<br/>
https://arxiv.org/abs/1710.05488</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9010">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第3层</span>
            <span class="date">April 14th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9010">
<p>看了公众号的链接，貌似有点艰辛～</p><p>单看wgan的话，其实不用微分几何和Wasserstein距离的概念也能导出个大概。</p><p>我感觉概率图这条路已经够艰辛了，然而概率图还没学好，就不大想探索其他路子了。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9012">
        <div class="comment-header">
            <span class="author">Mobil</span>
            <span class="layer reply">第4层</span>
            <span class="date">April 15th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9012">
<p>苏老师，您行的：) 最近看了您的GAN, Capsule, 黎曼几何深入浅出的分析，受益匪浅。非常感谢您！</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-9030">
        <div class="comment-header">
            <span class="author">sinpen</span>
            <span class="layer">第3层</span>
            <span class="date">April 17th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-9030">
<p>关于求边缘概率的最大似然函数的公式（2），我有些没有看懂。这里考虑的边缘概率是关于变量$x$的对吧？所以是$\int{q(x|z)q(z)}dz$。对于这个边缘概率分布，其似然函数为什么不是（以下离散表示）<br/>
$$\sum_{x} \ln(\int{q(x|z)q(z)dz}) $$<br/>
而是<br/>
$$\sum_{x} \widetilde{p}(x) \ln(\int{q(x|z)q(z)dz})$$</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9031">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第4层</span>
            <span class="date">April 17th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9031">
<p>$\tilde{p}(x)$是$x$真实的分布，它不一定能写出来，但它是真实存在的。</p><p>如果要写成$\sum\limits_{x} \ln(\int{q(x|z)q(z)dz})$，其实还要声明<br/>
$$\sum_{x} \ln(\int{q(x|z)q(z)dz}),\quad x\sim \tilde{p}(x)$$<br/>
这样一来，也就相当于<br/>
$$\mathbb{E}_{x\sim \tilde{p}(x)}\left[\ln(\int{q(x|z)q(z)dz})\right]$$<br/>
也就等价于<br/>
$$\sum_{x} \tilde{p}(x)\ln(\int{q(x|z)q(z)dz})$$</p><p>可以进一步参考：<br/>
<a href="https://kexue.fm/archives/5343#" target="_blank">https://kexue.fm/archives/5343#</a>数值计算vs采样计算</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9032">
        <div class="comment-header">
            <a href="http://zixuanweeei.xin" class="author" target="_blank">sinpen</a>
            <span class="layer reply">第5层</span>
            <span class="date">April 18th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9032">
<p>ok,thx.</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-9546">
        <div class="comment-header">
            <span class="author">Rey</span>
            <span class="layer">第4层</span>
            <span class="date">July 31st, 2018</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-9546">
<p>苏老师，你好。<br/>
在这里我有一点疑问就是在loss里边有一项算p(z|x)和q(z)的KL散度。但q(z)是标准正态分布，那通过训练得到的p(z|x)也是接近标准正态分布的，那么它应该不是那种小方差的紧凑正态分布啊，这样只采样一次还成立吗？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9547">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第5层</span>
            <span class="date">July 31st, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9547">
<p>p(z|x)事实上不接近标准正态分布。</p><p>KL散度和重构loss不能割裂开来看，两者加起来才是最终的loss，这样一来KL散度这一项就不能足够接近于0，因为还要保证重构效果。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-16633">
        <div class="comment-header">
            <span class="author">Glory</span>
            <span class="layer reply">第6层</span>
            <span class="date">June 13th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-16633">
<p>你好，请问评论是需要经过筛选嘛？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-16634">
        <div class="comment-header">
            <span class="author">Glory</span>
            <span class="layer reply">第7层</span>
            <span class="date">June 13th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-16634">
<p>Okay, 我换用Safari而非Chrome就可以提交评论了。</p><p>我的问题是，输出$q(\mathbf{z}|\mathbf{x})$的方差$\mathbf{\sigma}^2(\mathbf{x})$的均值，发现可以比较接近1。直观上想，想要采样生成的图像比较像training data，$q(\mathbf{z}|\mathbf{x})$张成modes应该比较好的cover $\mathcal{N}(\mathbf{0}, I)$，而不能有大片的空隙，故方差不应太小。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-16642">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第8层</span>
            <span class="date">June 15th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-16642">
<p>一般来说只要你样本达到几万张甚至更多的时候，$\sigma(\boldsymbol{x})$就会接近于0。大小是相对的，不留空隙也不一定要很大的$\sigma(\boldsymbol{x})$，况且还有没训练到理想情况的可能性。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-9642">
        <div class="comment-header">
            <span class="author">小火火</span>
            <span class="layer">第5层</span>
            <span class="date">August 23rd, 2018</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-9642">
<p>苏老师，您好！看您的文章后思路清晰不少，但对采样这个过程还是有点疑惑。q(x|z)是生成器，服从高斯分布，请问采样时隐变量z是怎么确定的呢？这个z在生成图像时具体指什么呢？怎么将图像和概率联系起来？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9645">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第6层</span>
            <span class="date">August 24th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9645">
<p>目的就是将图像分布映射为高斯分布，从而从高斯分布采样一个点就可以生成一个图像。其余不大理解你说什么～</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9648">
        <div class="comment-header">
            <span class="author">小火火</span>
            <span class="layer reply">第7层</span>
            <span class="date">August 24th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9648">
<p>请问这个采样的点是随机选择的吗？如果是随机选择，为什么会生成我们所要求的图像？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-9649">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第8层</span>
            <span class="date">August 24th, 2018</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-9649">
<p>当然是随机选择的。</p><p>比如我们说“随机选一个人脸”，或者说“随机选一个点”，虽然地球上的人是有限的，但原则上人脸的样子是无限的，而点也是无限的。在某些条件下，这些“点”和“人脸”可以构成一个映射关系（甚至是一一映射），并且可以尝试找出这个映射出来。</p><p>这样一来，只要我们随机选一个点，我们就可以找出对应的那个点的人脸。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-11047">
        <div class="comment-header">
            <span class="author">杨七七</span>
            <span class="layer">第6层</span>
            <span class="date">April 23rd, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-11047">
<p>老师您好，感觉您写的很好，见解很独特，但是我有一个问题，VAE究竟是有监督的还是无监督的呢。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-11069">
        <div class="comment-header">
            <span class="author">后卫</span>
            <span class="layer">第7层</span>
            <span class="date">April 28th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-11069">
<p>学习到了好多，我想问下，如果每个几层就设置这样一个vae的均值方差估计层是否可行呢？<br/>
就相当于每个神经元都是一个正太分布，加上随机的抖动。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-11074">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第8层</span>
            <span class="date">April 28th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11074">
<p>可以试验，效果无法预估。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-11218">
        <div class="comment-header">
            <span class="author">fractal</span>
            <span class="layer">第8层</span>
            <span class="date">May 23rd, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-11218">
<p>既然只采样一个，为什么不直接假设狄拉克分布</p><p>此时x和z之间“几乎”具有一一对应关系，接近确定的函数x=μ(z)。<br/>
“- -对应”说法是不是有点问题，双射才能说一一对应吧。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-11222">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第9层</span>
            <span class="date">May 23rd, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11222">
<p>都说是“几乎”一一对应了，我又没说完全一一对应，有什么问题？</p><p>只采样一个是因为后验高斯分布比较接近狄拉克分布，只采样一个就能达到比较好的精度。为什么不直接假设狄拉克分布？假设狄拉克分布能算下去吗？都是无穷大了...</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-11225">
        <div class="comment-header">
            <span class="author">fractal</span>
            <span class="layer reply">第10层</span>
            <span class="date">May 24th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11225">
<p>可能我理解的有问题吧，如果u不可逆就不能给说是一一对应吧？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-11231">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第11层</span>
            <span class="date">May 25th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11231">
<p>我就不明白你跟我较什么劲，我说了是“几乎”一一对应，不是一一对应，你是不懂“几乎”是什么意思吗？如果可逆我干嘛还要加个“几乎”？</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-11814">
        <div class="comment-header">
            <span class="author">wq</span>
            <span class="layer">第9层</span>
            <span class="date">August 13th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-11814">
<p>苏神您好，请问公式(2)中括号里面的无法显示求解，那是怎么从公式(2)推导到公式(3)的呢？我看了您之前的文章是通过求解联合分布的KL散度得到的公式(3)，但是在这边是怎么得到的呢？希望苏神能解答一下，非常感谢！</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-11825">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第10层</span>
            <span class="date">August 15th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11825">
<p>(3)不是由(2)推出的，本文的这部分内容只是在重新陈述《变分自编码器（二）》的结果，而不是推导。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-11972">
        <div class="comment-header">
            <span class="author">Wp</span>
            <span class="layer">第10层</span>
            <span class="date">September 10th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-11972">
<p>苏神，关于$(7)$式的采样问题，您在文中说的是$batch\_size$个样本$x_1, x_2,...,x_{batch\_size}$争夺$k$个点$z_1,..,z_k$. 我觉得(7)式的实现过程应该是对每个样本$x_i$. 我们都会采$k$个样本呀。这样实现的时候应该是一个$batch$，我们采$batch\_size * k$个$z$。</p><p>因为对每个$x_i$，我们都有$k$个样本。感觉效果不好是因为, 应该是$z$采样是随机的，而不是争夺。</p><p>就好像VAE实现的时候一样，我们对$batch\_size$个样本，不也是从$batch\_size$个专属分布$P(Z|X)$中各采一个点，从而有$batch\_size$个点$z_1,z_2,...,z_{batch\_size}$吗。没有对一个$batch$只采$1$个$z$吧。。</p><p>不知道哪里理解地有问题。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-11984">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第11层</span>
            <span class="date">September 12th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11984">
<p>你可以采样batch_size * k个$z$，但这样训练成本会增加很多，不过即便如此，效果依然不会好。你说的采样的随机的，依然只是表面原因，本质原因是这样采样出来的估计是有偏的，甚至可能偏得很严重，导致效果不行。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-12275">
        <div class="comment-header">
            <span class="author">Daisy</span>
            <span class="layer reply">第12层</span>
            <span class="date">October 30th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-12275">
<p>您好，最近在看您的文章，启发很大，非常感谢！</p><p>感觉楼上的问题是，在VAE中每个batch有n个x,并且对应的采样n个z，因此应该不存在多个争夺一个的情况。</p><p>所以，比较疑惑的是您说的采样一个点，指的是一个x采样一个z,还是一个batch采样一个z?</p><p>还有一个问题是那个\mu(z) = x,指的是所有sample的均值，还是一个sample的？感觉这里有点奇怪，我问的可能也有点问题，希望能解答一下，感谢！</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-12280">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第13层</span>
            <span class="date">October 31st, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-12280">
<p>$\mu(z)=x$指的是输入为$z$、输出为$x$的函数，所以自然是指单个样本的均值。</p><p>其余疑问，请联系上下文、咬文嚼字地反复阅读10遍本文。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-25867">
        <div class="comment-header">
            <span class="author">winter</span>
            <span class="layer reply">第12层</span>
            <span class="date">November 30th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-25867">
<p>苏老师，“本质原因是这样采样出来的估计是有偏的”这句话是指公式$(7)$中的$\ln \left(\frac{1}{k} \sum_{i=1}^k q\left(x \mid z_i\right)\right)$是公式$(6)$中$\ln \left(\mathbb{E}_{z \sim q(z)}[q(x \mid z)]\right)$的有偏估计吗?</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-25888">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第13层</span>
            <span class="date">December 2nd, 2024</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-25888">
<p>是的</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-11974">
        <div class="comment-header">
            <span class="author">Wp</span>
            <span class="layer">楼主</span>
            <span class="date">September 10th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-11974">
<p>还有伯努利分布，几乎采样都是均值$u(z)$. 这是为什么?</p><p>是因为收敛后的模型，对应的伯努利分布的参数，它们要不就接近$0$, 要不就接近$1$吗？</p> 
</div>
        </div>
    </div>
    <div class="comment reply" id="comment-11985">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第2层</span>
            <span class="date">September 12th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11985">
<p>你按概率采样也无妨呀～习惯上输出均值而已，因为均值是确定性的结果。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-11975">
        <div class="comment-header">
            <span class="author">Wp</span>
            <span class="layer">第2层</span>
            <span class="date">September 11th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-11975">
<p>就好像后面您提到的IWAE一样，指出当$k$很大，采样多时。$p(z|x)$形式就不重要了。我感觉本质上从专属的条件分布$p(z|x)$采样的原因吧。。而因为$(7)$式是从$q(z)$中采样，不是专属的，所以对一个$x_i$采再多$z$也没用。而不是说$batch\_size$个$x$竞争$k$个$z$的混战问题。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-11986">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第3层</span>
            <span class="date">September 12th, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-11986">
<p>如果你能采样无穷多个点，又或者直接把积分$\int q(z)q(x|z)dz$积出来，那是没有问题的</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-12287">
        <div class="comment-header">
            <span class="author">mch</span>
            <span class="layer">第3层</span>
            <span class="date">November 2nd, 2019</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-12287">
<p>请问一下博主，（2）式是怎么得到的？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-12290">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第4层</span>
            <span class="date">November 2nd, 2019</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-12290">
<p>最大似然</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-13030">
        <div class="comment-header">
            <span class="author">B.Q.Xu</span>
            <span class="layer">第4层</span>
            <span class="date">March 18th, 2020</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-13030">
<p>苏神您好。看了您的文章之后受益匪浅，非常感谢～<br/>
但是有两处疑惑，希望您能不吝赐教。<br/>
1.「粗略地看，这只是对隐变量空间做了平移和缩放，所以方差也可以不大。」 这一句的因果关系我不太明白。为什么如此便可推导或者猜测出方差不大呢？<br/>
2. 损失函数的下界在实操中应该如何计算呢？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-13033">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第5层</span>
            <span class="date">March 18th, 2020</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-13033">
<p>1. 指的是原来的隐变量均值可能不为0，方差不为1，通过整体平移和缩放就可以达到均值为0、方差为1的目标；</p><p>2. 不明白什么叫做“实操中应该如何计算”。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-14141">
        <div class="comment-header">
            <span class="author">wjw</span>
            <span class="layer">第5层</span>
            <span class="date">August 22nd, 2020</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-14141">
<p>我们直接把均值网络μ(z)的结果就当成x。而能这样做，表明q(x|z)是一个方差很小的正态分布。</p><p>苏神，请问在哪里能保证q(x|z)是一个方差很小的正态分布？不好意思，实在想不明白。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-14151">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第6层</span>
            <span class="date">August 24th, 2020</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-14151">
<p>没有严格的理论保证，只能说实验结果显示。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-15344">
        <div class="comment-header">
            <span class="author">ErickZhou</span>
            <span class="layer">第6层</span>
            <span class="date">January 23rd, 2021</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-15344">
<p>苏老师，您写的“重参之神”这个部分中，"于是z求导就不再是0，μ(x),σ(x)终于可以获得属于它们的反馈了"这句话不理解。我主要对如何获得反馈，是什么样的反馈不明白。您能否说明一下，或者给一下参考资料(我大概是缺乏这样的知识，不明白什么样的层才能进入反向传播）</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-15350">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第7层</span>
            <span class="date">January 23rd, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-15350">
<p>能求出非零导数就是可导的，否则就是不可导的。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-15355">
        <div class="comment-header">
            <span class="author">ErickZhou</span>
            <span class="layer reply">第8层</span>
            <span class="date">January 24th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-15355">
<p>谢谢您的回复</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-15369">
        <div class="comment-header">
            <span class="author">ErickZhou</span>
            <span class="layer">第7层</span>
            <span class="date">January 26th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-15369">
<p>苏老师，您这儿公式3后面的句子“通过最小化L来分别找出p(x|z)和q(x|z)。”，是不是笔误，应该是“通过最小化L来分别找出p(z|x)和q(x|z)。”，这样分别对应编码部分p(z|x)，和解码部分q(x|z)。</p><p>还有一个问题请教您，关于公式(2)。公式2中ln后面这些应该是∫q(x|z)q(z)dz=∫q(x,z)dz=q(x)，这个是边缘概率吧，我查到边缘概率的似然函数好像就是包括参数theta的边缘概率，比如写成q(x,theta)，然后求解时为了方便，前面取ln。请问您，公式2为什么除了这个lnq(x)，前面还有∫p(x)？怎么和边缘概率的最大似然函数对应？谢谢您。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-15381">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第8层</span>
            <span class="date">January 26th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-15381">
<p>1. 确实是笔误，已经修正；</p><p>2. 带积分才是似然函数，不需要怎么对应。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-17264">
        <div class="comment-header">
            <span class="author">Julian</span>
            <span class="layer">第8层</span>
            <span class="date">September 4th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-17264">
<p>其实，为什么一个点足够，https://arxiv.org/abs/1606.05908v2这篇文章解释得更加简单，却也能让我信服。如果每次采用多个点，那么就相当于BGD，然而耗时，如果我只采样一个点，也就是SGD的标准设置了。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-17273">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第9层</span>
            <span class="date">September 6th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-17273">
<p>前提是无偏估计量，否则采样一个点就跟采样多个点，就不是SGD跟BGD的区别了。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-17449">
        <div class="comment-header">
            <span class="author">JayChung</span>
            <span class="layer">第9层</span>
            <span class="date">September 28th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-17449">
<p>对“资源争夺战”的解释，我觉得不够有说服力。我觉得Eq(7)比Eq(3)差的原因，也就是为什么不能从p(z) sample一些点z去估计q(x|z)，而要从p(z|x)去sample z，是因为对于大部分z, q(x|z)趋于零。从q(x|z) sample点去估计是一种importance sampling的方法。参考EM算法的理解: https://www.zhihu.com/question/40797593/answer/275171156</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-17458">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第10层</span>
            <span class="date">September 28th, 2021</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-17458">
<p>这跟“资源争夺战”并不矛盾呀。对于给定的$x$，$q(x|z_1),q(x|z_2),\cdots,q(x|z_k)$都很小，也很难优化，所以它优先选择最大的一个（假设为$z_1$）$q(x|z_1)$来优化，相当于$x$争夺到了$z_1$，模型开始让$z_1$去匹配$x$。但是下一轮我们又采样别的$z$了，又要重新开始匹配，导致每次匹配的效应无法累积。</p><p>事实上，有一种训练方案是可能成功的，就是随机采样一批$z$后，然后把它固定，按照$(7)$训练一段时间，再考虑重新采样，这样匹配的效应可以累积起来，虽然效果也不会太好，但不至于完全训练不出来。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-18344">
        <div class="comment-header">
            <span class="author">叶Fuyin</span>
            <span class="layer">第10层</span>
            <span class="date">January 31st, 2022</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-18344">
<p>看过一些基于VAE的工作基本都是假定q(z|x)是服从相互独立的高斯分布，然后通过编码器得到mean和var，再进行采样，如果各个隐变量之间不是相互独立的，那有什么处理方式吗？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-18358">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第11层</span>
            <span class="date">February 2nd, 2022</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-18358">
<p>有是有，比如Inverse Autoregressive Flow等工作，还有 <a href="https://kexue.fm/archives/7574" target="_blank">https://kexue.fm/archives/7574</a> 这里介绍的NVAE。</p><p>但是，我们同时也要清楚，假设$p(z|x)$变量之间相互独立，一方面是“不得已而为之”，另一方面其实也是“故意为之”，因为假设隐变量之间相互独立，也是在迫使模型学会解耦隐变量，而这也是我们同时认为良好的隐变量所具备的特征，因为解耦之后我们才能更方便地进行比较或者编辑。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-19353">
        <div class="comment-header">
            <span class="author">shiketsu</span>
            <span class="layer">楼主</span>
            <span class="date">June 25th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-19353">
<p>苏老师，<br/>
您这篇文章中提到<br/>
“VAE希望通过隐变量分解来描述数据X的分布<br/>
p(x)=∫p(x|z)p(z)dz,p(x,z)=p(x|z)p(z)(1)<br/>
然后对p(x,z)用模型q(x|z)拟合”</p><p>上篇文章《变分自编码器（二）：从贝叶斯观点出发》（<a href="https://kexue.fm/archives/5343/comment-page-5" target="_blank">https://kexue.fm/archives/5343/comment-page-5</a>）中又说<br/>
“<br/>
但事实上，直接来对p(x,z)进行近似是最为干脆的。具体来说，定义p(x,z)=p̃(x)p(z|x)，我们设想用一个联合概率分布q(x,z)来逼近p(x,z)<br/>
”</p><p>我实在是没有读明白，究竟是用q(x,z)逼近p(x,z)，还是用q(x|z)逼近p(x,z)呢？</p><p>还望指教，非常感谢</p> 
</div>
        </div>
    </div>
    <div class="comment reply" id="comment-19355">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第2层</span>
            <span class="date">June 26th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-19355">
<p>打错了，是“$p(x|z)$用模型$q(x|z)$拟合”才对，谢谢指出。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-19529">
        <div class="comment-header">
            <span class="author">josh00</span>
            <span class="layer">第2层</span>
            <span class="date">July 24th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-19529">
<p>苏老师您好，公式（5）中，lnp（z｜x）怎么算出来呢？具体操作难道是Decoder之后再放一个encoder，然后生成一个对应的z，最小化当前z和之后z的均值？<br/>
在讲到这一步的时候，没有说lnp（z｜x）也是一个高斯分布。如果假设所有的分布都是高斯分布，是不是对于高斯分布有点太通用了？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-19541">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第3层</span>
            <span class="date">July 27th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-19541">
<p>$\ln p(z|x)$是给出$p(z|x)$的概率密度后，直接取对数得到的。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-19530">
        <div class="comment-header">
            <span class="author">josh00</span>
            <span class="layer">第3层</span>
            <span class="date">July 24th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-19530">
<p>此外，我个人认为公式中p和q有些混用？例如公式（4）中p(z|x)其实是未知的 我们真正计算只能使用q（z｜x），所以我们最小化的其实是KL（q（z｜x），q（z））？总感觉有哪里不太对，希望您不吝赐教。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-19539">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第4层</span>
            <span class="date">July 27th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-19539">
<p>没有混用，包括你前一个问题，答案也是一样的。</p><p>首先，确定出发点，在笔者关于VAE的推导体系中，没有“近似后验分布”这回事，请先忘记传统的贝叶斯推断体系。我们并不是给定$q(x|z)$和$q(z)$，然后想办法估计$q(z|x)$，这样的思想在本博客没有出现过。</p><p>划重点，我们是先构造了$p(x,z)=p(z|x)\tilde{p}(x)$，然后再去构造$q(x,z)=q(x|z)q(z)$，这两个构造是毫无关系的，毫无关系的，毫无关系的！没有谁是谁的近似。然后，我们去最小化$KL(p(x,z)\Vert q(x,z))$，目的是希望能实现$p(x,z)=q(x,z)$。</p><p>所以，整个过程是“先构造两个互不相关的分布”、“然后让它们主动地去相互接近”。所以不要问“$q(x|z)$和$q(z)$给定后对应$q(z|x)$是不是高斯分布”这种问题，我们从来没有说过它是，我们是希望两者能够主动地相互逼近。不是$p(x,z)$去逼近$q(x,z)$，也不是$q(x,z)$去逼近$p(x,z)$，是两者主动相互靠近。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-19762">
        <div class="comment-header">
            <span class="author">我爱学习</span>
            <span class="layer">第4层</span>
            <span class="date">September 8th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-19762">
<p>请问苏神，本文中的数据集 $X$ 投影到一个低维隐变量空间。<br/>
如果隐变量空间维度和数据集 $X$ 的样本数一致，对应 [变分自编码器（一）](<a href="https://kexue.fm/archives/5253" target="_blank">https://kexue.fm/archives/5253</a>) 图 vae是为每个样本构造专属的正态分布，然后采样来重构，那么 amortized inference 对应的是 变分自编码器（一）中图vae的传统理解吗？</p><p>在实现代码https://github.com/bojone/vae/blob/master/vae_keras.py中看到的是 z_mean, z_logvar 分别由两个 Dense 层从 intermediate_dim 投影到 latent_dim。感觉更类似 变分自编码器（一）中图vae的传统理解。不知道苏神对VAE vs Amortized VAE有什么看法？<br/>
https://erdogdu.github.io/csc412/slides/w11/sld11-1.pdf p12 页<br/>
不是很确定代码实现的 z_mean 和 z_logvar 对应 p12 的 ${N}\left(\mu_{\phi}\left(x_{i}\right), \Sigma_{\phi}\left(x_{i}\right)\right)$ 吗？<br/>
似乎所有的 VAE 代码都没有用让每个样本 x 对应一个专属的正态分布，而是 embedding 到 latent_dim，无法避免 amortization gap [Inference Suboptimality in Variational Autoencoders](https://arxiv.org/pdf/1801.03558.pdf)<br/>
谢谢苏神!</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-19775">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第5层</span>
            <span class="date">September 8th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-19775">
<p>我看了你给的pdf链接，感觉有点莫名其妙。</p><p>应该说，pdf里边所谓的“Standard VAE”，才是我说的“传统理解”，而它笔下的“Amortized VAE”，才是VAE的正确理解。但是吧，其实它的“Amortized VAE”，才是一般文献里边的“Standard VAE”，反而它自己描述的所谓“Standard VAE”，我才是第一次见...</p><p>z_mean, z_logvar 分别由两个 Dense 层从 intermediate_dim 投影到 latent_dim，Dense层的输入是显然依赖于$x$的，所以很明显对应的是Amortized VAE。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-20063">
        <div class="comment-header">
            <span class="author">Lily</span>
            <span class="layer">第5层</span>
            <span class="date">October 12th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-20063">
<p>请问苏老师，我对于公式2还是有一些不理解。在我的概念里，最大似然求得是使事件发生概率最大的模型参数，比如a是高斯模型是均值，那求的是使p(x|a)最大的a，这样理解有问题吗？如果没有问题的话，为什么公式2求的是使q(x)的期望最大的q(x|z)呢（这里的x是被认为服从p(x)分布对吗？）？这里面好像没有模型的参数，而且条件概率怎么可以认为是哪个参数呢？我想来想去感觉似懂非懂的，还请有时间的时候帮忙解答，谢谢！</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-20069">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第6层</span>
            <span class="date">October 12th, 2022</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-20069">
<p>$q(x|z)$就是待求模型，如果将它参数化，就是求$q(x|z)$中的模型参数。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-21761">
        <div class="comment-header">
            <span class="author">stico</span>
            <span class="layer">第6层</span>
            <span class="date">May 25th, 2023</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-21761">
<p>请问苏神，你在（6）（7）式中是说明直接采样q(x|z)是不可行的，但是我有点不理解，gan是否是采用这种方式训练的</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-21766">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第7层</span>
            <span class="date">May 25th, 2023</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-21766">
<p>不是</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-23908">
        <div class="comment-header">
            <span class="author">wwz</span>
            <span class="layer">第7层</span>
            <span class="date">March 11th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-23908">
<p>感谢苏老师精彩的文章！我对这句“理论上我们还要从q(x|z)中再采样一次才得到x。但是，我们并没有这样做，我们直接把均值网络μ(z)的结果就当成x。”仍有疑惑，我的想法中最终计算的||x-u(z)||2里的x是原始图像，但把u(z)当做x就成为了||u(z)-u(z)||缺少了与原始图像的计算。才疏学浅，望苏老师解惑！</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-23919">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第8层</span>
            <span class="date">March 12th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-23919">
<p>这句话指的是推理过程，是先有了$\mu(z)$，再有$\Vert x-\mu(z)\Vert^2$或者$q(x|z)$，然后从$q(x|z)$采样，最后认为从$q(x|z)$中采样的结果就是$\mu(z)$（当方差足够小时，近似成立）</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-23909">
        <div class="comment-header">
            <span class="author">wwz</span>
            <span class="layer">第8层</span>
            <span class="date">March 11th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-23909">
<p>我还有一个疑问是，阅读您《从贝叶斯观点出发》的文章后，我知道了对于p(x|z)的生成模型近似，使用了高斯分布。然而在我的理解中，一般的极大似然会从总体里取多个样本估计固定但未知的参数，也就是x通常是多个采样，但文章中对z是否要多次采样的说明是我有些疑惑，如果<br/>
z多次采样，其参数不就不再固定了，那好像就无法估计了。才疏学浅，打扰苏老师了。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-23920">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第9层</span>
            <span class="date">March 12th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-23920">
<p>首先要转换一下逻辑：</p><p>1、在本系列推导中，不存在“最大似然”这个东西，而是取而代之最小化两个分布的KL散度；</p><p>2、最大似然等价于最小化边缘分布的KL散度，但不好算；</p><p>3、而我发现，VAE就是在最小化联合分布的KL散度，这应该是VAE最简单的推导方法；</p><p>4、联合分布的KL散度，公式为$\iint p(z|x)\tilde{p}(x) \log \frac{p(z|x)\tilde{p}(x)}{q(x|z)q(z)}dxdz$，其中最外边的积分也就是期望，可以改为采样，但注意这是一个联合分布，$(x,z)$本质上是平权的、成对出现的，所以$x,z$出现次数相等是很合理的。</p><p>当然，也有一些工作是加大$z$的采样次数的，比如 <a href="https://kexue.fm/archives/8791" target="_blank">https://kexue.fm/archives/8791</a> 介绍的重要性加权自编码器，它会加强decoder，但弱化encoder。</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-23922">
        <div class="comment-header">
            <span class="author">wwz</span>
            <span class="layer reply">第10层</span>
            <span class="date">March 12th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-23922">
<p>原来如此，谢谢苏老师！我都明白了。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-24447">
        <div class="comment-header">
            <span class="author">Renat</span>
            <span class="layer">第9层</span>
            <span class="date">May 29th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-24447">
<p>苏老师您好！想就本文请教一个小问题，后面提到的 IWAE ，相较于原始 VAE 训练过程中对 $z\sim p(z|x)$ 多次采样，除了理论出发点不同，从而前者可以解除对 $p(z|x)$ 的先验设定以外，是不是没有实质性的区别？如果是这样，您在系列之二中提到 “实验过采样多个的情形，感觉生成的样本并没有明显变化”，那 IWAE 这套框架在什么情况下会明显提升样本生成的效果呢？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-24457">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第10层</span>
            <span class="date">May 29th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-24457">
<p>IWAE可以进一步参考：<a href="https://kexue.fm/archives/8791" target="_blank">https://kexue.fm/archives/8791</a> ，它可能在采样数目非常多的时候才会有提升吧？我也没有深入实验过，判断生成样本没有明显变化也是凭肉眼的，没有定量计算过指标。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-24886">
        <div class="comment-header">
            <span class="author">XJTUNR</span>
            <span class="layer">第10层</span>
            <span class="date">July 23rd, 2024</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-24886">
<p>苏老师您好！想请教如下两个问题：<br/>
1.公式（5）中 $lnq(z)$这一项是不是不含参数，实际实现中可以忽略。<br/>
2.您文中提到，假设$q(x|z)$和$p(z|x)$都是方差很小的高斯分布（后验的先验）。$q(x|z)$我比较好理解，直接对均值计算MSE损失，可以认为是没有随机性。但是我们在推导VAE损失的之后，是希望$p(z|x)$与标准高斯分布的KL散度接近于0，这样$p(z|x)$的方差会接近于1，这和我们的先验是否矛盾呢？可以把VAE理解为推导的时候是分布于分布之间的对应，但是实现的时候是点对点的对应么？</p> 
</div>
        </div>
    </div>
    <div class="comment reply reply-deep" id="comment-24898">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第11层</span>
            <span class="date">July 24th, 2024</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-24898">
<p>1、$q(z)$的$q$是没有参数，但是$z$经过重参数后，$z$是有参数的，所以不可忽略；</p><p>2、我们是希望通过最小化“$p(z|x)$与标准高斯分布的KL散度”作为额外的正则项，来实现$p(z|x)$的边缘分布（也就是全体$z$的分布$p(z)$）接近标准高斯分布，并不是希望$p(z|x)$本身接近标准高斯分布。</p> 
</div>
        </div>
    </div>
</div>
<div class="comment-thread">
    <div class="comment" id="comment-27088">
        <div class="comment-header">
            <span class="author">Suahi</span>
            <span class="layer">楼主</span>
            <span class="date">March 12th, 2025</span>
        </div>
        <div class="comment-content">
            <div class="ListContent" id="comment-27088">
<p>您好苏神，本文中您提到一句话“而对于后验分布$p(z|x)$中，我们假设了它也是一个正态分布。既然前面说z与x几乎是一一对应的，那么这个性质同样也适用验分布$p(z|x)$，这就表明后验分布也会是一个方差很小的正态分布”，我有如下两个问题想请教：<br/>
1. $p(z|x)$如果是一个方差很小的正态分布，那是否不符合《变分自编码器（二）：从贝叶斯观点出发》一文中，“VAE还让所有的$p(Z|X)$都向标准正态分布看齐”这个假设，因为根据式$p(z)=\int p(x,z)dx=\int p(z|x)p(x)dx$可知，如果$p(z|x)$不服从甚至不接近$N(0,I)$分布，那么先验分布$p(z)$一定不符合甚至偏离$N(0,I)$分布，那么后期将VAE作为生成模型时从$N(0,I)$中随机采样放入解码器中就不恰当了；<br/>
2. 本文中提到“$q(x|z)$只是一个概率分布，我们从$q(z)$中采样出$z$后，代入$q(x|z)$后得到$q(x|z)$的具体形式，理论上我们还要从$q(x|z)$中再采样一次才得到x。但是，我们并没有这样做，我们直接把均值网络$μ(z)$的结果就当成$x$。而能这样做，表明$q(x|z)$是一个方差很小的正态分布”这句话是否有些自洽？因为并没有解释直接把$μ(z))$作为$x$的理由，而是直接把这个作为先决条件去推测$q(x|z)$以及$p(z|x)$的概率分布形态（小方差）是否有些不合理？<br/>
由于本人数学水平并不高明，上述是我思考后提出的两点拙见，希望能得到您的解答，这对我理解VAE原理非常有帮助！谢谢！</p> 
</div>
        </div>
    </div>
    <div class="comment reply" id="comment-27106">
        <div class="comment-header">
            <a href="https://kexue.fm" class="author admin" target="_blank">苏剑林</a>
            <span class="layer reply">第2层</span>
            <span class="date">March 13th, 2025</span>
        </div>
        <div class="comment-content">
            <div class="comment_content" id="comment-27106">
<p>1、“VAE还让所有的$p(Z|X)$都向标准正态分布看齐”是方法或者说途径，最终目标实际上是“$p(Z)$都向标准正态分布看齐”；</p><p>2、你这样反思也有道理，实际上直接输出$\mu(z)$更多的原因是为了去噪，因为你再采样一次的话，得到的可能会更加逼真，但也更多噪点的图片；</p><p>3、但是$p(x|z)$方差小，跟“采样一次”，两者应该是相互促进的结果，就是因为只采样了一次，然后加上重构损失会比KL损失大，所以模型学会了用这个采样一次的结果来重构，于是重构损失小了，KL损失相对就变大了，变大的原因之一就是$p(z|x)$方差小了。</p> 
</div>
        </div>
    </div>
</div>

        </main>
        
        <footer>
            <p>数据来源: <a href="https://spaces.ac.cn/archives/5383" target="_blank">科学空间</a></p>
            <p>本页面由爬虫脚本自动生成，包含原文所有评论</p>
        </footer>
    </div>
</body>
</html>
